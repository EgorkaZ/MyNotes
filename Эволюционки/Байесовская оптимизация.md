# Байесовская оптимизация

Особые алгоритмы [[EDA]]

## Описание

* Семейство методов оптимизации для случаев, когда функция приспособленности дорогостоящая
* Храним все сделанные пары запрос-приспособленность ($x_i, f(x_i)$)
* Работаем с параметризованным семейством вероятностных распределений: $P(\Theta) = P[f(x) = y | \Theta]$, где $\Theta$ - вектор параметров
	* Пытаемся предсказать, чему равна приспособленность в каком-то $x$ на данных параметрах $\Theta$
	* $\Theta$ должно занимать относительно немного места
	* $P(\Theta)$ должна быть достаточно простой, чтобы можно было считать условные вероятности для применения формулы Байеса
	* При этом $P(\Theta)$ должна достаточно хорошо описывать предметную область, задаётся из нашего представления о том, как устроена задача
* Есть формула Байеса: $$P(\Theta_v|(x_i,f(x_i))) = \frac{P((x_i,f(x_i))|\Theta_v)P(\Theta_v)}{\int P((x_i,f(x_i))|\Theta)P(\Theta)d\Theta}$$

### Схема t-ой итерации

* Берём (придуумываем) априорное распределение на параметрах, чтобы определить $P(\Theta)$
* По формуле Байеса на основе $P(\Theta)$ и пар $(x_i, f(x_i))_{i=1,...,t}$ строим апостериорное распределение $P(\Theta_t)$ по принципу максимального правдоподобия: максимизируем $P(\Theta_t|(x_i,f(x_i)))$ (решаем [[Задача оптимизации|задачу оптимизации]])
* На основе $P(\Theta_t)$ строим функцию $q(x)$ (acquisition function), определяющую "интересность" запросов в данный момент
	* Максимизация вероятности улучшения
	* Максимизация значения улучшения
	* Максимизация получаемой информации о функции
	* или что-то
* Оптимизируем $q(x)$ чем попало в зависимости от её вида
* Делаем запрос в получившейся точке минимума $x_t$