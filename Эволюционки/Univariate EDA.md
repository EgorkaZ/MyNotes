# "Одномерные" EDA

Алгоритм [[EDA]]

## Описание

* Модель == список вероятностных распределений аллелей для каждого гена
	* #def *Аллель* - набор значений, которые может принимать ген
* Каждый ген выбирается из своего распределения независимо
* Для [[Vector#Булевы|битовых строк]]: модель == список вероятностей выбрать бит-единицу

## Преимущества

* Просты в реализации
* Относительно просты для теоретических рассуждений
* Процедуры обновления распределений вычислительно просты

## Проблемы

* Не в состоянии обучиться взаимным зависимостям между генами
* Работают не так хорошо на несепарабельных задачах

## Примеры

### UMDA

( a.k.a. Univariate Marginal Distribution Algorithm )

#### Идея

* Как правило, описывается на битовых строках
* Генерируем выборку из `len` особей из нашего распределения
* Выбираем `bests` лучших особей из `len` согласно функции приспособленности
* `new_model = (1./bests) * samples.sum()`
	* $M_{t+1} = \frac{1}{\mu}\sum^{\mu}_{i=1}P_i$
	* То бишь, просто усреднение на всех особях

#### Особенности

* Предыдущая модель выбрасывается целиком
* Без специальных мер способен стагнировать, особенно при малых `bests`
* Чтобы это предотвратить, вероятности в модели обрезуются по инртервалу $[\frac{1}{n};1 - \frac{1}{n}]$
* При размере задачи $n$ (аналогично для других представлений)

### PBIL

( a.k.a. Population Based Incremental Learning )

Похож на [[#UMDA]]

#### Идея

* Генерируем выборку из `len` особей из нашего распределения
* Выбираем `bests` лучших особей из `len` согласно функции приспособленности
* Имеем коэффициент смешивания `mix`
	* Как правило, он значительный
* `new_model = (1. - mix) * prev_model + (mix/bests) * samples.sum()`

#### Особенности

* Скорость обучения `mix` определяет, насколько быстро модель обновляется
	* Похоже на температуру в [[Simulated annealing]]
* Если $mix = \Theta(\frac{1}{T})$, после $\Theta(T)$ итераций "знания" делятся на константный множитель


### CGA

( a.k.a. Compact Genetic Algorithm )

#### Идея

* Генерируем выборку из `len = 2` особей из нашего распределения
* Пусть `b` - лучшая из этих особей, а `w` - худшая
* `new_model = prev_model + mix(b - w)`
* Необходимо "поправить" вероятности в $M_{t+1}$ перед использованием, чтобы всё не выкатилось за $[0; 1]$

#### Особенности

* Скорость обучения `mix` определяет, насколько быстро модель обновляется
* Часто используется значение `mix = 1. / n`
* Значение `1 / mix = K` в литературе называют "размером популяции"
	* Это не тот размер популяции `len`, который на самом деле таковым является при выстраивании аналогий с другими алгоритмами
	* CGA конструировался как модель генетического алгоритма