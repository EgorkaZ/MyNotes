# Решение задачи линейной классификации

Решаем [[Задача линейной классификации|задачу линейной классификации]]

### Ключевая гипотеза

объекты (хорошо) разделимы

### Идея

Поиск среди разделяющих поверхностей, описываемых уравнением $f(x,w)=0$

### Margin

(функция отступа) для объекта $x_i$:
$$
M_i(w) = y_i f(x_i, w)
$$

$y_i \in \{+1, -1\}$ 

* $M_i(w) < 0$ - свидетельство того, что объект классифицирован некорректно
* Слишком большой margin говорит о том, что мы начинаем [[Проблема переобучения|переобучаться]]

#### Эмпирический риск

Количество объектов из $\mathcal{D}$, на которых классификатор $a_w$ допускает ошибки:
$$
\mathcal{L}(a_w, \mathcal{D}) = \mathcal{L}(w) = \sum^{|\mathcal{D}|}_i[M_i(w) < 0]
$$

* Чтобы искать параметры, надо снижать количество ошибок
* Функция не является гладкой => невозможен поиск экстремумов

#### Аппроксимация

$$
\widetilde{\mathcal{L}}(w) = \sum^{|\mathcal{D}|}_i \mathcal{L}(M_i(w))
$$

где $\mathcal{L}(M_i(w)) = \mathcal{L}(a_w(x_i, \mathcal{D}), x_i)$ - функция потерь

Нам нужна $\mathcal{L}$ такая, что она:
* неотрицательная
* невозрастающая
* гладкая

Варианты:
![[Pasted image 20220413204706.png]]
* $H(M) = (-M)_+$ - кусочно-линейная (правило Хебба)
* $V(M) = (1 - M)_+$ - кусочно-линейная (SVM)
* $L(M) = log_2(1 + e^{-M})$ - логарифмическая
* $Q(M) = (1 - M)^2$ - квадратичная
	* Не невозрастающая, но нам ок, потому что слишком большой [[#Margin]] тоже может значить, что всё плохо
* $S(M) = 2(1 + e^M)-1$ - сигмоидальная
* $E(M) = e^{-M}$ - экспоненциальная

### Линейный классификатор

* $f_j$:$X \rightarrow \mathbb{R}, j=1,\dots,n$ - численные признаки
* *$w_1,\dots,w_n \in \mathbb{R}$ - **веса** признаков

$$
a_w(x, \mathcal{D}) = sign(\sum_{i=1}^n w_i f_i(x) - w_0)
$$

Эквивалентно:
$$
a_w(x,\mathcal{D}) = sign(\langle w,x \rangle)
$$
если добавить признак $f_0(x) = -1$

Похоже на [[Нейрон#Нейрон МакКаллока-Питтса]]

#### Семейство линейных алгоритмов

В общем-то, все алгоритмы линейной классификации будут образовывать семейство $A_{linear}$

$$
A_{linear} = \{a_w(x) = sign(\langle w,x \rangle)| w \in \mathbb{R}\}
$$

#### Обучение

Найти вектор параметров $w$ (вектор даблов, по сути).

Нужен примерно любой [[Задача оптимизации#Алгоритмы|алгоритм оптимизации]], который сможет оптимизировать [[#Эмпирический риск]]
* Мы *знаем*, как устроена эта функция (она не чёрный ящик)
* В частности, мы знаем, что она является гладкой

Задача минимизации эмпирического риска: 
$$
\widetilde{\mathcal{L}}(w) = \sum^{|\mathcal{D}|}_i \mathcal{L}(M_i(w)) = \sum^{|\mathcal{D}|}_i\mathcal{L}(\langle w,x_i \rangle y_i) \rightarrow \min_w
$$

Т.е. хотим минимизировать $w$. Используем [[Градиентный спуск]], будем итеративно считать $w_k$:
$$
w_{k+1} = w_k - \mu\sum_i^{|\mathcal{D}|}\mathcal{L}'(\langle w_k,x_i \rangle y_i)x_i y_i
$$

См [[Градиентный спуск#Проблема]]