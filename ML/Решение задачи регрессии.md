# Решение задачи регрессии

Решаем [[Задача регрессии|задачу регрессии]]

### Ключевая гипотеза

Пусть $\Theta(x) = \Theta$ вокруг $x$ (т.е. константа вокруг точки):
$$
\mathcal{L}(\Theta,\mathcal{D}_{train}) = \sum_{i=1}^{|\mathcal{D}_{train}|} w_i(x)(\Theta - y_i)^2 \rightarrow \min_{\Theta \in \mathbb{R}}
$$

### Идея

Будем использовать ядерное сглаживание
* $K$ - [[Ядерная функция]]
* $h$ - ширина окна

$$
w_i(x) = K(\frac{\rho(x_i,x)}{h})
$$

### Ядерное сглаживание Надарая-Ватсона

$$
a_{NonParamReg\mathbb{h}}(x; \mathcal{D}_{train}; h; K) = {\large \frac{\sum_{x_i \in \mathcal{D}_{train}} y_i w_i(x)}{\sum_{x_i \in \mathcal{D}_{train}} w_i(x)}} = {\large \frac{\sum_{x_i \in \mathcal{D}_{train}} y_i K(\frac{\rho(x_i,x)}{h})}{\sum_{x_i \in \mathcal{D}_{train}} K(\frac{\rho(x_i,x)}{h})}}
$$

Формула скользящего среднего (чем бы оно ни было)

Можно провести ту же манипуляцию, что в [[Обобщённый метрический классификатор#С нефиксированной шириной окна]]

### Теорема

Формула работает. Там какие-то выкладки есть, мне лень. Дело было во 2-й лекции. #todo

### Анализ
[[kNN-like методы|Анализ]]