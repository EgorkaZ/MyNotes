# Регуляризация
Решаем [[Проблема переобучения|проблему переобучения]]

**Регуляризация** это некоторые ограничения, которые мы накладываем на параметр $\Theta$, которые представляют сложность или вероятность модели и могут быть формализованы с помощью некоторой функции $c(\Theta)$:
$$\Theta_{reg} = arg\min_{\Theta} \mathcal{L}(\Theta)+\alpha c(\Theta)$$

### Ключевая гипотеза

w "скачет", чем вызывает переобучение

### Идея

Ограничим норму w

Добавим штраф регуляризации для нормы весов:
$$
\mathcal{L}_\tau(a_w,\mathcal{D}) = \mathcal{L}(a_w,\mathcal{D}) + \frac{\tau}{2}||w||^2 \rightarrow \min_{w}
$$

* $\tau$ - коэффициент регуляризации
	* отражает баланс между качеством и обобщаемостью
* $\frac{\tau}{2}||w||^2$ - регуляризатор
	* некоторый штраф для поиска [[Решение задачи линейной классификации#Эмпирический риск|эмпирического риска]]

### Гребневая регуляризация
**Предположение**:
* значения вектора $\Theta$ имеют распределение Гаусса с ковариационной матрицей $\sigma I_n$: $$\mathcal{L}(\Theta) = ||F\Theta-y||^2+\frac{1}{2\sigma}||\Theta||^2 \rightarrow \min_\Theta$$
	* $\tau = \frac{1}{2\sigma}$ - коэффициент регуляризации
	* Внедрим её в решение задачи [[Матричное разложение#Метод наименьших квадратов|МНК]]: $$\Theta_\tau^*=(F^TF+\tau I_n)^{-1}F^\tau y$$
	$$
	\Theta_\tau^*=U(D^2+\tau I_n)^{-1}DV^Ty = \sum_{j=1}^n\frac{\sqrt{\lambda_j}}{\lambda_j+\tau}u_j(v_j^Ty)
  $$
    $$
	||\Theta||^2 = \sum_{j=1}^n\frac{1}{\lambda_j+\tau}(v_j^Ty)^2
  $$
  
  ### Лассо Тибширани
  
  **Предположение**:
  * значения вектора $\Theta$ имеют распределение Лапласа: $$
  \begin{cases}
  	\mathcal{L}(\Theta)=||F\Theta-y||^2 \rightarrow \min_\Theta \\
	\sum_{j=1}^n|\Theta_i|\le \kappa
  \end{cases}
  $$
  
  **LASSO** (least absolute shrinkage and selection operator)
  
  Результриующая задача оптимизации ([[Матричное разложение#Метод наименьших квадратов|МНК]]):
 $$
 \mathcal{L}(\Theta) = ||F\Theta-y||^2+\tau||\Theta||_1 \rightarrow \min_\Theta
 $$
 
 где
 * $||\Theta||_1$ - $l1$-норма 
 * $||\Theta||_1=\sum||\Theta_i||$

* Хорошего аналитического решения не существует
* Есть хорошее вычислительное решение

### Для градиентного спуска
Можно применить к [[Градиентный спуск|градиентному спуску]]

$$\nabla\mathcal{L}(w) = \nabla\mathcal{L}(w)+\tau w$$
$$w_{k+1} = w_k(1-\mu\tau)-\mu\nabla\mathcal{L}(w)$$

### Сравнение
![[Pasted image 20220417035300.png]]