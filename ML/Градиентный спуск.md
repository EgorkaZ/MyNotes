# Градиентный спуск

* $w_{0}$ - [[#Начальные значения|начальное значение]]
* $w_{k+1} = w_k - \mu\nabla\mathcal{L}(w_k)$ 
* $\mu$ - **шаг градиента** или **скорость сходимости**

### Проблемы

#### Жир

Если используем для обучения модельки на датасете, придётся считать функцию производной на каждом элементе датасета, а это дохрена

**Решение**: [[Стохастический градиентный спуск]]

#### Застревание
![[Pasted image 20220416225625.png]]

### Начальные значения

Варианты:
* $w_j = 0$ для всех $j=1\dots n$
	* __Очень__ не рекомендуется
* Маленьие случайные значения: $w_j \in [-\frac{1}{2n},\frac{1}{2n}]$
	* В простых случаях для [[Задача линейной классификации|линейной классификации]] самое оно
* $w_j = \frac{\langle y, f_j \rangle}{\langle f_j, f_j \rangle}$
* Обучение по небольшой начальной выборке
* Запускать из разных начальных приближений, выбор лучшего решения

### Эвристики на скорость сходимости

* Сходимость выпуклых функций достигается тогда, когда:
	* $\mu_k \rightarrow 0$
	* $\sum\mu_k \rightarrow \infty$
	* $\sum(\mu_k)^2 < \infty$
* **Наискорейший спуск**
	* $\mathcal{L}(w_k - \mu_k\nabla\mathcal{L}(w_k)) \rightarrow \min_{\mu_k}$
	* В общем, наиболее эффективен
* Шаги для "выпрыгивания" локальных минимумов
	* Если вы знаете природу функцию, будет хорошо
* Методы второго порядка
	* Гессианы дорогие(
* Использование среднего вектора недавних шагов

### Плюсы
* Простота реализации
* Легко обобщается для любых $f$ и $\mathcal{L}$
* Возможность динамического обучения
* Поддерживает сверхмалые выборки

### Минусы
(Ещё см. [[#Проблемы]])
* Медленная сходимость
* Застревание в локальных минимумах и седловых точках
* Очень важен правильный подбор эвристик
* Переобучение