# Обобщённый метрический классификатор

$$
a_{GenDistClassifier}(u; \mathcal{D}_{train}) = argmax_{y\in Y} \sum_{i=1}^{|\mathcal{D}_{train}|} [y(x_{(u,i)}=y)]w_{(i,u)}
$$

где $w_{(i,u)}$ - функция значимости (веса) $i$-го соседа $u$

Можем считать, что $\sum_{i=1}^{|\mathcal{D}_{train}|} [y(x_{(u,i)}=y)]w_{(i,u)}$ это оценка близости объекта $u$ к классу $y$

### Как выбираем $w$

$w_{(i,u)}$ это что-то из:

* Линейно убывающая функция
* Экспоненциально убывающая функция
* [[Ядерная функция]]

### Окно Парзена-Розенбалта

$K$ это [[Ядерная функция|ядерная функция]]

#### С фиксированной шириной окна

Вводим $h$ - **параметр нормировки** (ширина окна)

$$
a_{\Large GenDistClass\mathbb{h}}(u; \mathcal{D}_{train}; h; K) = argmax_{y\in Y} \sum_{i=1}^{|\mathcal{D}_{train}|} [y(x_{(u,i)}=y)]K(\frac{\rho(u, x_{(u,i)})}{h})
$$

* Здесь мы берём такую [[Ядерная функция|ядерную функцию]], чтобы в $1$ (или где-то) она уходила в $0$
* Для подбора $h$ всё ещё надо много телодвижений и понимаение, что у нас за данные. А мы ленивые

#### С нефиксированной шириной окна

В качестве $h$ для каждого отдельного объекта берём расстояние до $k+1$-го объекта. Как [[kNN]], только мы выдаём большие веса более близким объектам

$$
a_{\Large GenDistClass\mathbb{k}}(u; \mathcal{D}_{train}; h; K) = argmax_{y\in Y} \sum_{i=1}^{|\mathcal{D}_{train}|} [y(x_{(u,i)}=y)]K(\frac{\rho(u, x_{(u,i)})}{\rho(u,x_{(u,k+1)})})
$$

### Выбор (обучение) расстояния

Расстояние можно выбирать.
Пример (взвешенное [[Меры#Расстояние Минковского|расстояние Минковского]]):

$$
\mathcal{p}(x,y) = (\sum_i^n w_i|x_i - y_i|^{\mathcal{p}})^{\frac{1}{\mathcal{p}}}
$$

* "Обучение" этого расстояния - выбор оптимальных $w_i$
	* Т.е. каждой фиче из $n$ мы задаём вес
* Выбор ядер тоже можно назвать выбором расстояния

### Анализ
[[kNN-like методы|Анализ]]