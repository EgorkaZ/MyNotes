# MapReduce

## Мотивация

* Есть задача, которая достаточно легко решается на одной машине
	* Решение влезает в ~20 строчек кода
* Исходные данные не влезают на одну машину

## Наблюдение

Многие подобные задачи вписываются в map-reduce

```haskell
map :: Document -> [(Key, Value)]

reduce :: Key -> [Value] -> Res
```

![[Pasted image 20220523172117.png]]

## Распределённая реализация

* Есть $M$ узлов-мапперов
	* Параллельно выполняют `map` для разных (скажем) документов
* И $R$ узлов-редьюсеров
	* Параллельно выполняют операцию `reduce` для разных ключей
* И один мастер для координации
* Все $kv$-пары, соответствующие определённом ключу, должны попадать на один $R$ узел
	* Скажем, `hash(k) % R`

![[Pasted image 20220523172344.png]]

### Исполнение запроса

* Мапперы начинают работать
* **Все** мапперы заканчивают работать
* Каждый редьюсер (независимо) переносит $kv$-пары с мапперов
* Перенеся все пары, редьюсер начинает работать
* **Все** редьюсеры заканчивают работать
* Запрос исполнен

![[Pasted image 20220523172541.png]]

### Мапперы

* Каждый маппер хранит в памяти $R$ корзин
	* Каждая держит $kv$ пары для попадания в конкретный редьюсер
	* Когда заканчивается память, сбрасываем корзины на диск, отсортировав по ключам
		* В это время редьюсер уже может выкачивать файл
* Иногда сообщаем статус мастеру

![[Pasted image 20220523172745.png]]

### Редьюсеры
* Читает предназначенные для него корзины с мапперов
	* Собирает предназначенные для него ключи
* Объединяет все полученные корзины
* Группирует результаты по ключу
* Для каждого ключа вызывает `reduce`
* На каждом редьюсере — свой файл с результатом

![[Pasted image 20220523173049.png]]

### Сбои

#### Сбой в рабочем узле

* Мастер проверяет доступность узлов пингом
* Если какой-то из узлов недоступен — перезапустим его подзадачу с нуля на другом узле
	* Выдаём идентификаторы задачам, чтобы не учесть ответ дважды
* В ходе вычислений мы не меняем никакие данные
	* Запустить одну задачу в двух копиях — не страшно

#### Сбой в маппере

* Маппер не смог отдать часть корзин 
	* Он может быть недетерминирован и перетасовать данные перед работой с ними
* Перезапустим всю задачу на новом маппере
* Выбросим то, что получили от старого

#### Сбой в мастере
* Перезапуск всей задачи с нуля
* Мастер может время от времени делать снапшоты
#todo

### Оптимизации

#### Локальность map

* Будем пытаться запускать `map`-задачу для файла F на том же узле, на котором этот файл лежит
	* Чтобы передавать меньше данных по сети
	* Узлов-кандидатов может быть несколько, потому что [[Replication|репликация]]
* Иногда не сможет получиться
* Будем выбирать ближайший узел

#### Избыточность

* Можем запускать несколько копий любой подзадаи на нескольких узлах
	* Брать результат той копии, которая завершится раньше
* Запускать все задачи в двух копиях — дорого
* Будем запускать копию подзадачи, которая выполняется медленно
	* Ускорим общее время выполнения
	* Т.к. медленные задачи не будут тормозить весь процесс
* Когда 95% задач выполнилось, раскопируем оставшиеся 5%
	* Это самые медленные задачи, которые тормозят нам всё
	* Процент можно настраивать

#### Каскады
 * Иногда задачу нельзя решить за один map-reduce
 * Представим задачу: для каждого интервала $[0; 1000)$, $[1000; 2000)$, $[2000; 3000)$ и т.д. сказать, сколько слов имеет встречаемость в данном интервале
 * Посчитаем задачу по интервалам
 * Посчитаем задачу над интервалами

В общем случае map-reduce задачи представляют из себя ориентированный ациклический граф. Так что используем топсорт

![[Pasted image 20220523174840.png]]

* Отправляем задачу без зависимостей исполняться параллельно на кластер
* Когда получаем решение, убираем зависимость у всех задач, которые её требовали
* Пробем снова

#### Combiner
* `map` можт выдать много пар на один ключ
	* Например, много одинаковых слов в тексте
	* Приходится передавть много лишнего
* Применим локальную свёртку
	* Эта операция называется _Combiner_
* Зачастую то же самое, что `reduce`
	* Но не всегда: например, считаем среднее от чего-нибудь

![[Pasted image 20220523175238.png]]

##### Per bucket

* Применяем _Combiner_ перед записью очередной корзины для $k$-ого редьюсера на диск
* Экономим меньше трафика
* Зато редьюсеры могут забирать старые корзины, не дожидаясь конца работы маппера

![[Pasted image 20220523175525.png]]

##### Total

* Применяем _Combiner_  после того, как все корзины записаны
* Но редьюсеры приходят только после завершения работы маппера

![[Pasted image 20220523175642.png]]

## А зачем

Вообще, довольно естественна идея предподсчитать часть результата на каждом узле, а затем поагрегировать

![[Pasted image 20220523180039.png]]
